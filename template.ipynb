{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template for DS3000 Final data analysis project. Once you finish, please remove all my instructions. You do not need to exactly follow the structure in the template but please make sure you have all the components. Write your report in paragraphs. Only use bullet points when list something (eg: functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3000 Final Project \n",
    "#### Team number: 9\n",
    "- Xavier Yu, Vilasini Nathan, Justin Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pylab as py\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import re, time, json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdunction\n",
    "\n",
    "- One or two paragraphs about the background of the project. eg: the backgound of PalWorld and why your analysis can be interesting\n",
    "- State your research questions. Limit the number of research questions to be one or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Data Source\n",
    "\n",
    "- List the website you have scraped the data from.\n",
    "- List which information you have scraped\n",
    "- Describe what kind of cleaning you have done to the data\n",
    "\n",
    "### Webscraping and cleaning functions overview\n",
    "\n",
    "List all the functions you have written for webscraping and data cleaning. For each one, write one sentence to describe it. \n",
    "- `extract_soup()`\n",
    "    - build url and return soup object\n",
    "\n",
    "### Data overview\n",
    "\n",
    "- Show a couple of rows of the cleaned data you are going to use for the analysis\n",
    "- Which is your target value (if exists)\n",
    "- Give a general summary about the other features\n",
    "- Discuss if there is any potential problems about the data (eg: missing values, any features that you did not collect but may be important, any other concerns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for webscraping and cleaning. Make sure write full \n",
    "# docstrings for each function\n",
    "def extract_soup():\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Retrieve HTML content from a given URL\n",
    "\n",
    "    Args:\n",
    "        url (str): The webpage URL to fetch HTML from\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pal_links(html):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of links to pals from the HTML soup object.\n",
    "    \n",
    "    Args:\n",
    "        html (string): html text of a webpage\n",
    "        \n",
    "    Returns:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "    \"\"\"\n",
    "    # parse HTML content to create soup\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    pal_links = []  # Create an empty list to store full URLs\n",
    "\n",
    "    # Find all <div> elements with class pal\n",
    "    for pal_div in soup.find_all('div', class_='pal'):\n",
    "        \n",
    "        # Inside each <div class='pal'>, find the first <a> tag\n",
    "        a_tag = pal_div.find('a')\n",
    "        \n",
    "        # Check if the <a> tag exists and has an 'href' attribute\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            # Build the full link by appending the relative path to the base URL\n",
    "            full_link = 'https://palworld.gg' + a_tag['href']\n",
    "            pal_links.append(full_link)\n",
    "\n",
    "    return pal_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pal_data(pal_links):\n",
    "    \"\"\"\n",
    "    Given a list of Palworld page links, fetch each page and extract the pal's name and stats.\n",
    "\n",
    "    Args:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "\n",
    "    Returns:\n",
    "        pal_data (dictionary): dictionary containing a pal's name, link, and stats.\n",
    "    \"\"\"    \n",
    "    name, href, stats = [], [], []\n",
    "\n",
    "    for link in pal_links:\n",
    "        # Request the HTML content of each pal's page\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        pal_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Get the pal's name from the <h1> tag\n",
    "        name_tag = pal_soup.find('h1')\n",
    "        pal_name = name_tag.text.strip() if name_tag else 'Unknown'\n",
    "\n",
    "        # Dictionary to store stats like HP, Attack, etc.\n",
    "        stats_dict = {}\n",
    "\n",
    "        # Look for the section containing stats\n",
    "        stats_div = pal_soup.find('div', class_='stats')\n",
    "        if stats_div:\n",
    "            items_div = stats_div.find('div', class_='items')\n",
    "            if items_div:\n",
    "                # Loop through all stat items\n",
    "                for item in items_div.find_all('div', class_='item'):\n",
    "                    stat_name_tag = item.find('div', class_='name')\n",
    "                    stat_value_tag = item.find('div', class_='value')\n",
    "                    if stat_name_tag and stat_value_tag:\n",
    "                        stat_name = stat_name_tag.text.strip()\n",
    "                        stat_value = stat_value_tag.text.strip()\n",
    "                        stats_dict[stat_name] = stat_value\n",
    "\n",
    "        # Save the collected data into corresponding lists\n",
    "        name.append(pal_name)\n",
    "        href.append(link)\n",
    "        stats.append(stats_dict)\n",
    "        \n",
    "    pal_data = {'Name': name, 'Link': href, 'Stats': stats}\n",
    "\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rarity_info(pal_data):\n",
    "    \"\"\"\n",
    "    Adds rarity_level and rarity_name to each pal dictionary\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): contains pal info with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Rarity Level' and 'Rarity Name' fields added\n",
    "    \"\"\"\n",
    "    # empty lists to store rarity level and rarity name\n",
    "    rar_level, rar_name = [], []\n",
    "    \n",
    "    for link in pal_data['Link']:\n",
    "        html = get_html(link)  # FIXED: now using each Pal's page\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the rarity div\n",
    "        rarity_div = soup.find('div', class_='rarity')\n",
    "        rarity_level, rarity_name = None, None\n",
    "        if rarity_div:\n",
    "            lv_tag = rarity_div.find('div', class_='lv')\n",
    "            name_tag = rarity_div.find('div', class_='name')\n",
    "            if lv_tag and name_tag:\n",
    "                rarity_level = lv_tag.text.strip()\n",
    "                rarity_name = name_tag.text.strip()\n",
    "\n",
    "        rar_level.append(rarity_level)\n",
    "        rar_name.append(rarity_name)\n",
    "        \n",
    "    pal_data['Rarity Level'] = rar_level\n",
    "    pal_data['Rarity Name'] = rar_name\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_element_work(pal_data):\n",
    "    \"\"\"\n",
    "    Adds element and work suitability to pal_data\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): Dictionary, with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Element', 'Work Suitability' added\n",
    "    \n",
    "    \"\"\"\n",
    "    # empty lists to store element and work suitability\n",
    "    element, work = [], []\n",
    "    \n",
    "    # iterate through all links\n",
    "    for link in pal_data['Link']:\n",
    "        \n",
    "        # get HTML\n",
    "        html = get_html(link)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        # to store all elements of each pal\n",
    "        pal_element = []\n",
    "        \n",
    "        # get contents of first div with class elements\n",
    "        elements_div = soup.find('div', class_='elements')\n",
    "\n",
    "        # get text from each element and append to pal_element\n",
    "        for el in elements_div.find_all('div', class_='name'):\n",
    "                    element_text = el.text.strip()\n",
    "                    pal_element.append(element_text)\n",
    "        \n",
    "        # empty dict to store work suitability and level\n",
    "        work_level = {}\n",
    "\n",
    "        # get contents of first div with class works\n",
    "        works_div = soup.find('div', class_='works')\n",
    "\n",
    "        # iterates through contents of work_div with div and class item\n",
    "        for item in works_div.find_all('div', class_='item'):\n",
    "            \n",
    "            # only extracts displayed items\n",
    "            if 'display:none' not in item.get('style', ''):\n",
    "                if 'Lv' in item.text:\n",
    "                    work_suit, level = item.text.split('Lv')\n",
    "                    work_level[work_suit] = int(level)\n",
    "                else:\n",
    "                    work_suit = item.text.strip()\n",
    "                    work_level[work_suit] = ''\n",
    "        \n",
    "        # appends pal_element and work_level to the larger element and work lists\n",
    "        element.append(pal_element)\n",
    "        work.append(work_level)\n",
    "    \n",
    "    # adds to dictionary\n",
    "    pal_data['Element'] = element\n",
    "    pal_data['Work Suitability'] = work\n",
    "    \n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://palworld.gg/pals'\n",
    "html = get_html(url)\n",
    "pal_links = extract_pal_links(html)\n",
    "\n",
    "pal_data = fetch_pal_data(pal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_rarity_info(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_element_work(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Stats</th>\n",
       "      <th>Rarity Level</th>\n",
       "      <th>Rarity Name</th>\n",
       "      <th>Element</th>\n",
       "      <th>Work Suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anubis</td>\n",
       "      <td>https://palworld.gg/pal/anubis</td>\n",
       "      <td>{'HP': '120', 'Defense': '100', 'Crafting Spee...</td>\n",
       "      <td>10</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Earth]</td>\n",
       "      <td>{'Handiwork': 4, 'Mining': 3, 'Transporting': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arsox</td>\n",
       "      <td>https://palworld.gg/pal/arsox</td>\n",
       "      <td>{'HP': '85', 'Defense': '95', 'Crafting Speed'...</td>\n",
       "      <td>4</td>\n",
       "      <td>Common</td>\n",
       "      <td>[Fire]</td>\n",
       "      <td>{'Deforesting': 1, 'Kindling': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astegon</td>\n",
       "      <td>https://palworld.gg/pal/astegon</td>\n",
       "      <td>{'HP': '100', 'Defense': '125', 'Crafting Spee...</td>\n",
       "      <td>9</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dragon, Dark]</td>\n",
       "      <td>{'Handiwork': 1, 'Mining': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azurmane</td>\n",
       "      <td>https://palworld.gg/pal/azurmane</td>\n",
       "      <td>{'HP': '130', 'Defense': '110', 'Crafting Spee...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Electricity]</td>\n",
       "      <td>{'Gathering': 1, 'Generating Electricity': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Azurobe</td>\n",
       "      <td>https://palworld.gg/pal/azurobe</td>\n",
       "      <td>{'HP': '110', 'Defense': '100', 'Crafting Spee...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Water, Dragon]</td>\n",
       "      <td>{'Watering': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Wumpo Botan</td>\n",
       "      <td>https://palworld.gg/pal/wumpo-botan</td>\n",
       "      <td>{'HP': '140', 'Defense': '110', 'Crafting Spee...</td>\n",
       "      <td>8</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Leaf]</td>\n",
       "      <td>{'Deforesting': 3, 'Handiwork': 2, 'Planting':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Xenogard</td>\n",
       "      <td>https://palworld.gg/pal/xenogard</td>\n",
       "      <td>{'HP': '110', 'Defense': '130', 'Crafting Spee...</td>\n",
       "      <td>9</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dragon]</td>\n",
       "      <td>{'Mining': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Xenolord</td>\n",
       "      <td>https://palworld.gg/pal/xenolord</td>\n",
       "      <td>{'HP': '130', 'Defense': '120', 'Crafting Spee...</td>\n",
       "      <td>8</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dark, Dragon]</td>\n",
       "      <td>{'Gathering': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Xenovader</td>\n",
       "      <td>https://palworld.gg/pal/xenovader</td>\n",
       "      <td>{'HP': '90', 'Defense': '85', 'Crafting Speed'...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Dark]</td>\n",
       "      <td>{'Deforesting': 2, 'Transporting': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Yakumo</td>\n",
       "      <td>https://palworld.gg/pal/yakumo</td>\n",
       "      <td>{'HP': '85', 'Defense': '85', 'Crafting Speed'...</td>\n",
       "      <td>4</td>\n",
       "      <td>Common</td>\n",
       "      <td>[Normal]</td>\n",
       "      <td>{'Gathering': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name                                 Link  \\\n",
       "0         Anubis       https://palworld.gg/pal/anubis   \n",
       "1          Arsox        https://palworld.gg/pal/arsox   \n",
       "2        Astegon      https://palworld.gg/pal/astegon   \n",
       "3       Azurmane     https://palworld.gg/pal/azurmane   \n",
       "4        Azurobe      https://palworld.gg/pal/azurobe   \n",
       "..           ...                                  ...   \n",
       "220  Wumpo Botan  https://palworld.gg/pal/wumpo-botan   \n",
       "221     Xenogard     https://palworld.gg/pal/xenogard   \n",
       "222     Xenolord     https://palworld.gg/pal/xenolord   \n",
       "223    Xenovader    https://palworld.gg/pal/xenovader   \n",
       "224       Yakumo       https://palworld.gg/pal/yakumo   \n",
       "\n",
       "                                                 Stats Rarity Level  \\\n",
       "0    {'HP': '120', 'Defense': '100', 'Crafting Spee...           10   \n",
       "1    {'HP': '85', 'Defense': '95', 'Crafting Speed'...            4   \n",
       "2    {'HP': '100', 'Defense': '125', 'Crafting Spee...            9   \n",
       "3    {'HP': '130', 'Defense': '110', 'Crafting Spee...            7   \n",
       "4    {'HP': '110', 'Defense': '100', 'Crafting Spee...            7   \n",
       "..                                                 ...          ...   \n",
       "220  {'HP': '140', 'Defense': '110', 'Crafting Spee...            8   \n",
       "221  {'HP': '110', 'Defense': '130', 'Crafting Spee...            9   \n",
       "222  {'HP': '130', 'Defense': '120', 'Crafting Spee...            8   \n",
       "223  {'HP': '90', 'Defense': '85', 'Crafting Speed'...            7   \n",
       "224  {'HP': '85', 'Defense': '85', 'Crafting Speed'...            4   \n",
       "\n",
       "    Rarity Name          Element  \\\n",
       "0          Epic          [Earth]   \n",
       "1        Common           [Fire]   \n",
       "2          Epic   [Dragon, Dark]   \n",
       "3          Rare    [Electricity]   \n",
       "4          Rare  [Water, Dragon]   \n",
       "..          ...              ...   \n",
       "220        Epic           [Leaf]   \n",
       "221        Epic         [Dragon]   \n",
       "222        Epic   [Dark, Dragon]   \n",
       "223        Rare           [Dark]   \n",
       "224      Common         [Normal]   \n",
       "\n",
       "                                      Work Suitability  \n",
       "0     {'Handiwork': 4, 'Mining': 3, 'Transporting': 2}  \n",
       "1                    {'Deforesting': 1, 'Kindling': 2}  \n",
       "2                        {'Handiwork': 1, 'Mining': 4}  \n",
       "3        {'Gathering': 1, 'Generating Electricity': 4}  \n",
       "4                                      {'Watering': 3}  \n",
       "..                                                 ...  \n",
       "220  {'Deforesting': 3, 'Handiwork': 2, 'Planting':...  \n",
       "221                                      {'Mining': 3}  \n",
       "222                                   {'Gathering': 1}  \n",
       "223              {'Deforesting': 2, 'Transporting': 2}  \n",
       "224                                   {'Gathering': 2}  \n",
       "\n",
       "[225 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Palworld Weapons Scraper with Selenium\n",
    "=======================================\n",
    "Functions to scrape weapon data from palworld.gg/items\n",
    "by using the website's filter functionality\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict, Optional\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def setup_driver(headless: bool = False) -> webdriver.Chrome:\n",
    "    \"\"\"\n",
    "    Set up Chrome driver with options.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    headless : bool\n",
    "        Run browser in headless mode (no GUI)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    webdriver.Chrome\n",
    "        Configured Chrome driver\n",
    "    \"\"\"\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    # Initialize the driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def scrape_palworld_weapons_selenium(headless: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape weapon data from Palworld using Selenium to interact with filters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    headless : bool\n",
    "        Run browser in headless mode (default: False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing weapon information\n",
    "    \"\"\"\n",
    "    driver = None\n",
    "    try:\n",
    "        print(\"Setting up browser driver...\")\n",
    "        driver = setup_driver(headless)\n",
    "        \n",
    "        print(\"Navigating to Palworld items page...\")\n",
    "        driver.get(\"https://palworld.gg/items\")\n",
    "        \n",
    "        # Wait for page to load\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        \n",
    "        print(\"Waiting for initial page load...\")\n",
    "        # Wait for items to appear first\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"item\")))\n",
    "        time.sleep(2)\n",
    "        \n",
    "        print(\"Waiting for filter dropdown to load...\")\n",
    "        # Find and interact with the item type dropdown\n",
    "        dropdown = wait.until(\n",
    "            EC.element_to_be_clickable((By.ID, \"item_type\"))\n",
    "        )\n",
    "        \n",
    "        # Store initial count\n",
    "        initial_items = len(driver.find_elements(By.CLASS_NAME, \"item\"))\n",
    "        print(f\"Initial items on page: {initial_items}\")\n",
    "        \n",
    "        print(\"Selecting 'Weapon' from filter...\")\n",
    "        # Use JavaScript to set the value and trigger change event\n",
    "        driver.execute_script(\"\"\"\n",
    "            var dropdown = document.getElementById('item_type');\n",
    "            dropdown.value = 'EPalItemTypeA::Weapon';\n",
    "            dropdown.dispatchEvent(new Event('change', { bubbles: true }));\n",
    "        \"\"\")\n",
    "        \n",
    "        # Alternative: Use Select (uncomment if JS doesn't work)\n",
    "        # select = Select(dropdown)\n",
    "        # select.select_by_value(\"EPalItemTypeA::Weapon\")\n",
    "        \n",
    "        print(\"Waiting for page to update with ALL weapons...\")\n",
    "        # Wait for the page to change\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Wait until pagination disappears or item count changes significantly\n",
    "        try:\n",
    "            # Check if pagination is gone (indicates all items on one page)\n",
    "            pagination_gone = wait.until_not(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"page-nav\")),\n",
    "                timeout=5\n",
    "            )\n",
    "        except:\n",
    "            # If pagination doesn't disappear, just wait for items to load\n",
    "            pass\n",
    "        \n",
    "        # Additional wait to ensure all weapons are loaded\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Scroll to bottom to trigger any lazy loading\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Count the items after filtering\n",
    "        weapon_items = driver.find_elements(By.CLASS_NAME, \"item\")\n",
    "        print(f\"Found {len(weapon_items)} items after filtering for weapons\")\n",
    "        \n",
    "        # Verify we have weapons by checking a few items\n",
    "        weapon_count = 0\n",
    "        for item in weapon_items[:5]:  # Check first 5 items\n",
    "            try:\n",
    "                type_div = item.find_element(By.CLASS_NAME, \"type\")\n",
    "                if \"Weapon\" in type_div.text:\n",
    "                    weapon_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if weapon_count > 0:\n",
    "            print(f\"Confirmed: Found weapons in the filtered results\")\n",
    "        else:\n",
    "            print(\"Warning: No weapons found in sample check, but continuing...\")\n",
    "        \n",
    "        # Now parse all the results\n",
    "        print(\"Extracting weapon data...\")\n",
    "        page_source = driver.page_source\n",
    "        weapons_df = parse_weapons_from_selenium(page_source)\n",
    "        \n",
    "        print(f\"Successfully scraped {len(weapons_df)} weapons\")\n",
    "        return weapons_df\n",
    "        \n",
    "    except TimeoutException as e:\n",
    "        print(f\"Timeout: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "            print(\"Browser closed\")\n",
    "\n",
    "\n",
    "def wait_until_not(condition, timeout=10):\n",
    "    \"\"\"Helper function to wait until a condition is NOT met\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        try:\n",
    "            condition()\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def parse_weapons_from_selenium(html_content: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse weapon data from the filtered HTML content.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_content : str\n",
    "        The HTML content after filtering for weapons\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing weapon information\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    weapons_data = []\n",
    "    \n",
    "    # Find all item divs (should all be weapons after filtering)\n",
    "    items = soup.find_all('div', class_='item')\n",
    "    \n",
    "    for item in items:\n",
    "        weapon_info = {}\n",
    "        \n",
    "        # Get the weapon name\n",
    "        name_div = item.find('div', class_='text')\n",
    "        if name_div:\n",
    "            weapon_info['Name'] = name_div.text.strip()\n",
    "        \n",
    "        # Find the item-card div which contains the details\n",
    "        item_card = item.find('div', class_='item-card')\n",
    "        if item_card:\n",
    "            # Find all key-value pairs\n",
    "            keys = item_card.find_all('div', class_='key')\n",
    "            for key in keys:\n",
    "                key_text = key.find('div', class_='text')\n",
    "                value_div = key.find('div', class_='value')\n",
    "                \n",
    "                if key_text and value_div:\n",
    "                    key_name = key_text.text.strip()\n",
    "                    value = value_div.text.strip()\n",
    "                    \n",
    "                    # Extract the specific metrics we want\n",
    "                    if key_name == 'Price':\n",
    "                        try:\n",
    "                            weapon_info['Price'] = int(value)\n",
    "                        except:\n",
    "                            weapon_info['Price'] = value\n",
    "                    elif key_name == 'Physical Attack':\n",
    "                        try:\n",
    "                            weapon_info['Physical Attack'] = int(value)\n",
    "                        except:\n",
    "                            weapon_info['Physical Attack'] = value\n",
    "                    elif key_name == 'Durability':\n",
    "                        try:\n",
    "                            weapon_info['Durability'] = int(value)\n",
    "                        except:\n",
    "                            weapon_info['Durability'] = value\n",
    "        \n",
    "        # Only add if we have a name\n",
    "        if 'Name' in weapon_info:\n",
    "            weapons_data.append(weapon_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(weapons_data)\n",
    "    \n",
    "    # Ensure all columns exist, fill missing with None\n",
    "    for col in ['Name', 'Price', 'Physical Attack', 'Durability']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[['Name', 'Price', 'Physical Attack', 'Durability']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Alternative functions for manual HTML parsing (if Selenium is not available)\n",
    "\n",
    "def extract_weapons_from_saved_html(html_file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract weapons from a manually saved HTML file where weapons filter\n",
    "    was already applied in the browser.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_file_path : str\n",
    "        Path to the HTML file (saved after applying weapon filter)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing weapon information\n",
    "    \"\"\"\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    return parse_weapons_from_selenium(html_content)\n",
    "\n",
    "\n",
    "def scrape_palworld_weapons() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main function to scrape weapon data from Palworld.\n",
    "    This will try Selenium first, with fallback options.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing weapon information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try Selenium approach first\n",
    "        return scrape_palworld_weapons_selenium(headless=False)\n",
    "    except ImportError:\n",
    "        print(\"Selenium not installed. Please install it with: pip install selenium\")\n",
    "        print(\"Also ensure you have ChromeDriver installed and in PATH\")\n",
    "        print(\"\")\n",
    "        print(\"Alternative: Save the page manually after filtering for weapons,\")\n",
    "        print(\"then use: extract_weapons_from_saved_html('path/to/file.html')\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Selenium: {e}\")\n",
    "        print(\"\")\n",
    "        print(\"Alternative: Save the page manually after filtering for weapons,\")\n",
    "        print(\"then use: extract_weapons_from_saved_html('path/to/file.html')\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Palworld Weapons Summary + Export\n",
    "=================================\n",
    "Quick summary, validation, and CSV export of the scraped weapons data\n",
    "\"\"\"\n",
    "\n",
    "# 0) Run the scraper\n",
    "weapons_df = scrape_palworld_weapons()\n",
    "\n",
    "# 1) OPTIONAL: enforce numeric types for clean stats / export\n",
    "numeric_cols = [\"Price\", \"Physical Attack\", \"Durability\"]\n",
    "for c in numeric_cols:\n",
    "    if c in weapons_df.columns:\n",
    "        weapons_df[c] = pd.to_numeric(weapons_df[c], errors=\"coerce\")\n",
    "\n",
    "# 2) Summary\n",
    "print(\"=\" * 60)\n",
    "print(f\"TOTAL WEAPONS SCRAPED: {len(weapons_df)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not weapons_df.empty:\n",
    "    print(\"\\nBASIC STATISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(weapons_df[numeric_cols].describe().round(0))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FIRST 5 WEAPONS (HEAD):\")\n",
    "    print(\"-\" * 40)\n",
    "    display(weapons_df.head())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LAST 5 WEAPONS (TAIL):\")\n",
    "    print(\"-\" * 40)\n",
    "    display(weapons_df.tail())\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MISSING VALUES:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(weapons_df.isnull().sum())\n",
    "\n",
    "    # 3) EXPORT — write exactly what this dataframe contains\n",
    "    export_path = \"palworld_weapons.csv\"  # change if you want a different name/folder\n",
    "    weapons_df.to_csv(export_path, index=False)\n",
    "    print(f\"\\nSaved {len(weapons_df)} rows to: {export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Visualization functions overview\n",
    "List all the functions you have written for visualization. For each one, write one sentence to describe it. \n",
    "- `make_hist()`\n",
    "    - Generate a histogram with given data and feature\n",
    " \n",
    "### Visualization results\n",
    "- Present 3-4 data visualizations.\n",
    "- For each visualization, you need to include title, xlabel, ylabel, legend (if necessary)\n",
    "- For each visualization, explain why you make this data visualization (how it related to your research question) and explain what you have learned from this visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for visualization. Make sure write full \n",
    "# docstrings for each function\n",
    "def make_hist(df, y_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Modeling functions overview\n",
    "List all the functions you have written for modeling. For each one, write one sentence to describe it. \n",
    "- `fit_linear()`\n",
    "    - fit a linear model to the data and output the r2, slope and intercept\n",
    "\n",
    "### Model results\n",
    "\n",
    "- Present 2-3 models for the analysis.\n",
    "- Explain any pre-processing steps you have done (eg: scaling, polynomial, dummy features)\n",
    "- For each model, explain why you think this model is suitable and what metrics you want to use to evaluate the model\n",
    "    - If it is a classification model, you need to present the confusion matrix, calculate the accuracy, sensitivity and specificity with cross-validation\n",
    "    - If it is a regression model, you need to present the r2 and MSE with cross-validation\n",
    "    - If it is a linear regression model/multiple linear regression model, you need to interpret the meaning of the coefficient with the full data\n",
    "    - If it is a decision tree model, you need to plot the tree with the full data\n",
    "    - If it is a random forest model, you need to present the feature importance plot with the full data\n",
    "    - If it is a PCA, you need to explain how to select the number of components and interpret the key features in the first two components\n",
    "    - If it is a clustering, you need explain how to select the number of clustering and summarize the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for modeling. Make sure write full \n",
    "# docstrings for each function\n",
    "def fit_linear(df, y_feat, x_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- One or two paragraphs to summarize your findings in the modeling sections and do the models answer your research question?\r",
    "- Any other potential thing you can do with the analysis (eg: include more features, get more data, try some other models etc.)\n",
    "- List the contribution for each group member."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.