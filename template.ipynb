{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template for DS3000 Final data analysis project. Once you finish, please remove all my instructions. You do not need to exactly follow the structure in the template but please make sure you have all the components. Write your report in paragraphs. Only use bullet points when list something (eg: functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3000 Final Project\n",
    "#### Team number: 9\n",
    "- Xavier Yu, Vilasini Nathan, Justin Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdunction\n",
    "\n",
    "- One or two paragraphs about the background of the project. eg: the backgound of PalWorld and why your analysis can be interesting\n",
    "- State your research questions. Limit the number of research questions to be one or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Data Source\n",
    "\n",
    "- List the website you have scraped the data from.\n",
    "- List which information you have scraped\n",
    "- Describe what kind of cleaning you have done to the data\n",
    "\n",
    "### Webscraping and cleaning functions overview\n",
    "\n",
    "List all the functions you have written for webscraping and data cleaning. For each one, write one sentence to describe it. \n",
    "- `extract_soup()`\n",
    "    - build url and return soup object\n",
    "\n",
    "### Data overview\n",
    "\n",
    "- Show a couple of rows of the cleaned data you are going to use for the analysis\n",
    "- Which is your target value (if exists)\n",
    "- Give a general summary about the other features\n",
    "- Discuss if there is any potential problems about the data (eg: missing values, any features that you did not collect but may be important, any other concerns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for webscraping and cleaning. Make sure write full \n",
    "# docstrings for each function\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "import pylab as py\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Retrieve HTML content from a given URL\n",
    "\n",
    "    Args:\n",
    "        url (str): The webpage URL to fetch HTML from\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def extract_pal_links(html):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of links to pals from the HTML soup object.\n",
    "    \n",
    "    Args:\n",
    "        html (string): html text of a webpage\n",
    "        \n",
    "    Returns:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "    \"\"\"\n",
    "    # parse HTML content to create soup\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    pal_links = []  # Create an empty list to store full URLs\n",
    "\n",
    "    # Find all <div> elements with class pal\n",
    "    for pal_div in soup.find_all('div', class_='pal'):\n",
    "        \n",
    "        # Inside each <div class='pal'>, find the first <a> tag\n",
    "        a_tag = pal_div.find('a')\n",
    "        \n",
    "        # Check if the <a> tag exists and has an 'href' attribute\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            # Build the full link by appending the relative path to the base URL\n",
    "            full_link = 'https://palworld.gg' + a_tag['href']\n",
    "            pal_links.append(full_link)\n",
    "\n",
    "    return pal_links\n",
    "\n",
    "\n",
    "def fetch_pal_data(pal_links):\n",
    "    \"\"\"\n",
    "    Given a list of Palworld page links, fetch each page and extract the pal's name and stats.\n",
    "\n",
    "    Args:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "\n",
    "    Returns:\n",
    "        pal_data (dictionary): dictionary containing a pal's name, link, and stats.\n",
    "    \"\"\"    \n",
    "    name, href, stats = [], [], []\n",
    "\n",
    "    for link in pal_links:\n",
    "        # Request the HTML content of each pal's page\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        pal_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Get the pal's name from the <h1> tag\n",
    "        name_tag = pal_soup.find('h1')\n",
    "        pal_name = name_tag.text.strip() if name_tag else 'Unknown'\n",
    "\n",
    "        # Dictionary to store stats like HP, Attack, etc.\n",
    "        stats_dict = {}\n",
    "\n",
    "        # Look for the section containing stats\n",
    "        stats_div = pal_soup.find('div', class_='stats')\n",
    "        if stats_div:\n",
    "            items_div = stats_div.find('div', class_='items')\n",
    "            if items_div:\n",
    "                # Loop through all stat items\n",
    "                for item in items_div.find_all('div', class_='item'):\n",
    "                    stat_name_tag = item.find('div', class_='name')\n",
    "                    stat_value_tag = item.find('div', class_='value')\n",
    "                    if stat_name_tag and stat_value_tag:\n",
    "                        stat_name = stat_name_tag.text.strip()\n",
    "                        stat_value = stat_value_tag.text.strip()\n",
    "                        stats_dict[stat_name] = stat_value\n",
    "\n",
    "        # Save the collected data into corresponding lists\n",
    "        name.append(pal_name)\n",
    "        href.append(link)\n",
    "        stats.append(stats_dict)\n",
    "        \n",
    "    pal_data = {'Name': name, 'Link': href, 'Stats': stats}\n",
    "\n",
    "\n",
    "    return pal_data\n",
    "\n",
    "\n",
    "def add_rarity_info(pal_data):\n",
    "    \"\"\"\n",
    "    Adds rarity_level and rarity_name to each pal dictionary\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): contains pal info with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Rarity Level' and 'Rarity Name' fields added\n",
    "    \"\"\"\n",
    "    # empty lists to store rarity level and rarity name\n",
    "    rar_level, rar_name = [], []\n",
    "    \n",
    "    for link in pal_data['Link']:\n",
    "        html = get_html(link)  # FIXED: now using each Pal's page\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the rarity div\n",
    "        rarity_div = soup.find('div', class_='rarity')\n",
    "        rarity_level, rarity_name = None, None\n",
    "        if rarity_div:\n",
    "            lv_tag = rarity_div.find('div', class_='lv')\n",
    "            name_tag = rarity_div.find('div', class_='name')\n",
    "            if lv_tag and name_tag:\n",
    "                rarity_level = lv_tag.text.strip()\n",
    "                rarity_name = name_tag.text.strip()\n",
    "\n",
    "        rar_level.append(rarity_level)\n",
    "        rar_name.append(rarity_name)\n",
    "        \n",
    "    pal_data['Rarity Level'] = rar_level\n",
    "    pal_data['Rarity Name'] = rar_name\n",
    "\n",
    "    return pal_data\n",
    "\n",
    "\n",
    "def add_element_work(pal_data):\n",
    "    \"\"\"\n",
    "    Adds element and work suitability to pal_data\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): Dictionary, with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Element', 'Work Suitability' added\n",
    "    \n",
    "    \"\"\"\n",
    "    # empty lists to store element and work suitability\n",
    "    element, work = [], []\n",
    "    \n",
    "    # iterate through all links\n",
    "    for link in pal_data['Link']:\n",
    "        \n",
    "        # get HTML\n",
    "        html = get_html(link)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        # to store all elements of each pal\n",
    "        pal_element = []\n",
    "        \n",
    "        # get contents of first div with class elements\n",
    "        elements_div = soup.find('div', class_='elements')\n",
    "\n",
    "        # get text from each element and append to pal_element\n",
    "        for el in elements_div.find_all('div', class_='name'):\n",
    "                    element_text = el.text.strip()\n",
    "                    pal_element.append(element_text)\n",
    "        \n",
    "        # empty dict to store work suitability and level\n",
    "        work_level = {}\n",
    "\n",
    "        # get contents of first div with class works\n",
    "        works_div = soup.find('div', class_='works')\n",
    "\n",
    "        # iterates through contents of work_div with div and class item\n",
    "        for item in works_div.find_all('div', class_='item'):\n",
    "            \n",
    "            # only extracts displayed items\n",
    "            if 'display:none' not in item.get('style', ''):\n",
    "                if 'Lv' in item.text:\n",
    "                    work_suit, level = item.text.split('Lv')\n",
    "                    work_level[work_suit] = int(level)\n",
    "                else:\n",
    "                    work_suit = item.text.strip()\n",
    "                    work_level[work_suit] = ''\n",
    "        \n",
    "        # appends pal_element and work_level to the larger element and work lists\n",
    "        element.append(pal_element)\n",
    "        work.append(work_level)\n",
    "    \n",
    "    # adds to dictionary\n",
    "    pal_data['Element'] = element\n",
    "    pal_data['Work Suitability'] = work\n",
    "    \n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://palworld.gg/pals'\n",
    "html = get_html(url)\n",
    "pal_links = extract_pal_links(html)\n",
    "\n",
    "pal_data = fetch_pal_data(pal_links)\n",
    "pal_data = add_rarity_info(pal_data)\n",
    "pal_data = add_element_work(pal_data)\n",
    "\n",
    "# Create DataFrame for downstream processing\n",
    "df = pd.DataFrame(pal_data)\n",
    "\n",
    "# Expand the dict in \"Stats\" to columns\n",
    "stats_df = df[\"Stats\"].apply(pd.Series)\n",
    "\n",
    "# Merge back and coerce numbers\n",
    "df = pd.concat([df.drop(columns=[\"Stats\"]), stats_df], axis=1)\n",
    "\n",
    "# Convert stat columns and rarity to numeric\n",
    "stat_cols = stats_df.columns.tolist()\n",
    "for c in stat_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"Rarity Level\"] = pd.to_numeric(df[\"Rarity Level\"], errors=\"coerce\")\n",
    "\n",
    "# Optional combined metric\n",
    "df[\"Total Stats\"] = df[stat_cols].sum(axis=1)\n",
    "\n",
    "# Drop rows missing target\n",
    "df = df.dropna(subset=[\"Rarity Level\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Visualization functions overview\n",
    "List all the functions you have written for visualization. For each one, write one sentence to describe it. \n",
    "- `make_hist()`\n",
    "    - Generate a histogram with given data and feature\n",
    " \n",
    "### Visualization results\n",
    "- Present 3-4 data visualizations.\n",
    "- For each visualization, you need to include title, xlabel, ylabel, legend (if necessary)\n",
    "- For each visualization, explain why you make this data visualization (how it related to your research question) and explain what you have learned from this visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for visualization. Make sure write full \n",
    "# docstrings for each function\n",
    "def make_hist(df, y_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df, x=\"Total Stats\", y=\"Rarity Level\", hue=\"Rarity Name\")\n",
    "plt.title(\"Total Stats vs Rarity Level\")\n",
    "plt.xlabel(\"Total Stats\")\n",
    "plt.ylabel(\"Rarity Level\")\n",
    "plt.legend(title=\"Rarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Modeling functions overview\n",
    "List all the functions you have written for modeling. For each one, write one sentence to describe it. \n",
    "- `fit_linear()`\n",
    "    - fit a linear model to the data and output the r2, slope and intercept\n",
    "\n",
    "### Model results\n",
    "\n",
    "- Present 2-3 models for the analysis.\n",
    "- Explain any pre-processing steps you have done (eg: scaling, polynomial, dummy features)\n",
    "- For each model, explain why you think this model is suitable and what metrics you want to use to evaluate the model\n",
    "    - If it is a classification model, you need to present the confusion matrix, calculate the accuracy, sensitivity and specificity with cross-validation\n",
    "    - If it is a regression model, you need to present the r2 and MSE with cross-validation\n",
    "    - If it is a linear regression model/multiple linear regression model, you need to interpret the meaning of the coefficient with the full data\n",
    "    - If it is a decision tree model, you need to plot the tree with the full data\n",
    "    - If it is a random forest model, you need to present the feature importance plot with the full data\n",
    "    - If it is a PCA, you need to explain how to select the number of components and interpret the key features in the first two components\n",
    "    - If it is a clustering, you need explain how to select the number of clustering and summarize the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for modeling. Make sure write full \n",
    "# docstrings for each function\n",
    "def fit_linear(df, y_feat, x_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "Xcols = stat_cols  # or stat_cols + [\"Total Stats\"] if you want to include the sum too\n",
    "\n",
    "single_results = []\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for col in Xcols:\n",
    "    X = df[[col]].values\n",
    "    y = df[\"Rarity Level\"].values\n",
    "    # cross-validated R^2\n",
    "    r2_scores = cross_val_score(LinearRegression(), X, y, scoring=\"r2\", cv=cv)\n",
    "    single_results.append({\n",
    "        \"Stat\": col,\n",
    "        \"CV_R2_mean\": np.mean(r2_scores),\n",
    "        \"CV_R2_std\": np.std(r2_scores)\n",
    "    })\n",
    "\n",
    "single_results_df = pd.DataFrame(single_results).sort_values(\"CV_R2_mean\", ascending=False)\n",
    "print(single_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[stat_cols].copy()\n",
    "y = df[\"Rarity Level\"].values\n",
    "\n",
    "# Train/test for a clean holdout evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, stat_cols)\n",
    "])\n",
    "\n",
    "reg_pipe = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "y_pred = reg_pipe.predict(X_test)\n",
    "print(\"Holdout R^2:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Permutation importance on the linear model\n",
    "perm = permutation_importance(reg_pipe, X_test, y_test, n_repeats=30, random_state=42)\n",
    "perm_importance = pd.DataFrame({\n",
    "    \"feature\": stat_cols,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "print(perm_importance)\n",
    "\n",
    "# RandomForestRegressor feature importance\n",
    "rf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=800, random_state=42))\n",
    "])\n",
    "rf.fit(X_train, y_train)\n",
    "rf_r2 = r2_score(y_test, rf.predict(X_test))\n",
    "\n",
    "print(\"RF Holdout R^2:\", rf_r2)\n",
    "\n",
    "rf_feat_imp = pd.Series(rf.named_steps[\"rf\"].feature_importances_, index=stat_cols).sort_values(ascending=False)\n",
    "print(rf_feat_imp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- One or two paragraphs to summarize your findings in the modeling sections and do the models answer your research question?\n",
    "- Any other potential thing you can do with the analysis (eg: include more features, get more data, try some other models etc.)\n",
    "- List the contribution for each group member."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
