{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 3000 HW 5 \n",
    "\n",
    "Due: Sunday July 20th @ 11:59 PM EST\n",
    "\n",
    "### Submission Instructions\n",
    "Submit this `ipynb` file and the a `PDF` file included with the coding results to Gradescope (this can also be done via the assignment on Canvas).  To ensure that your submitted files represent your latest code, make sure to give a fresh `Kernel > Restart & Run All` just before uploading the files to gradescope. \n",
    "\n",
    "**Notice that this is a group assignment. Each group only need to submit one copy and when you submit the work, please include everyone in your group.**\n",
    "\n",
    "### Tips for success\n",
    "- Start early\n",
    "- Make use of Piazza\n",
    "- Make use of Office hour\n",
    "- Remember to use cells and headings to make the notebook easy to read (if a grader cannot find the answer to a problem, you will receive no points for it)\n",
    "- Under no circumstances may one student view or share their ungraded homework or quiz with another student [(see also)](http://www.northeastern.edu/osccr/academic-integrity), though you are welcome to **talk about** (not show each other) the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project proposal\n",
    "\n",
    "For this course, we aim to complete a data analysis project about the the game [Palworld](https://en.wikipedia.org/wiki/Palworld). To help you start with the project, here are a couple of things you need to consider and work on to get a clean data for later analysis. \n",
    "\n",
    "To start with the project, please take some time to get familiar with the game. You don't need to play it but please at least know the basic terminologies, like what is a Pal. (And also, if you do play it, please do not spend too much time on it.)\n",
    "\n",
    "The two recommended database is [https://palworld.gg/](https://palworld.gg/) and [https://paldb.cc/en/](https://paldb.cc/en/). You can use either, or both, or some other database about the Palworld. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 (10 points)\n",
    "\n",
    "Please list 2-3 questions you may be interested to study with the Palworld database. It can be anything related in the game, like the Pals, items or constructions. Some potential question structures can be: \n",
    "- Are `A` and `B` related? How they are related?\n",
    "- Which features may affect `C`'s change?\n",
    "- If I need a higher `D`, which features may have a lower/higher value?\n",
    "- Based on `E` and `F`, which items/pals are similar?\n",
    "- I need to predict the value for `G`, which features I need to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which of the Pal's stats best predict the rarity of the Pal/which Pal is the best in each rarity group?\n",
    "   - Regression + Scatterplot\n",
    "\n",
    "   \n",
    "3. Is there a correlation between Pal work suitability and Pal element? How are they related?\n",
    "\n",
    "   \n",
    "5. What is the value of a weapon item? What is the correlation between physical attack and price? Durability and price?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 (20 points)\n",
    "\n",
    "Based on the questions we proposed in the part 1.1, what features we may need to include in the analysis? Check the websites, which website has those information? **You need to pick at least 8 features for analysis.** We recommend a mix of numerical (numbers etc.) and categorical (level etc.) features. Is there any other features that you think it may be important but hard to extract or find on the website (can be something in or not in the game)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eight features selected for analysis are Pal stats, Pal rarity, Pal element, Pal work suitability, item price, item durability, item weight, and item physical attack. The numerical features are Pal stats, item price, item durability, item weight, and item physical attack. Work suitability could technically also be considered numerical (there is a categorical and numerical aspect to this feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 (20 points)\n",
    "\n",
    "Suppose you do have all the features you mentioned in part 1.2. List 3-4 data visulizations you can make with those features. You do not need to make those visulizations here. Just describe the type of the visualizations (histogram, scatter plot etc. ), which features are involved, will there any hover data or color being added, and **discuss how these data visualizations may be related (or even answer) to your questions in part 1.1**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the Pal's stats best predict the rarity of the Pal/which Pal is the best in each rarity group?\n",
    "  1) A scatterplot where the x-axis is one of the Pal's stats or a combined number representing all of the stats and the y-axis is the Pal's rarity\n",
    "  2) A scatterplot where color represents the rarity and the x-axis and y-axis are two different stats that I can pull like attack and defense\n",
    "  Both of these visualizations allow us to see the relationship between various Pal stats and a Pal's rarity, and will provide insight into the predictive power of certain stats vs. others.\n",
    "\n",
    "Is there a correlation between Pal work suitability and Pal element? How are they related?\n",
    "  1) A heatmap which contains work suitabilities on the x-axis and elements on the y-axis. The shade of the heatmap will be dependent on the numerical average of a particular work suitability for Pal's with a particular element.\n",
    "  The heatmap will reveal insight into whether there are certain work suitabilities associated with certain elements. Using this heatmap, we could then visualize the relationship between a certain work suitability and certain element which have a darker shading (higher numerical average of work suitability), to see if there is an actual relationship.\n",
    "\n",
    "What is the value of a weapon item? What is the correlation between physical attack and price? Durability and price?\n",
    "  1) A scatter plot where the x-axis represents the physical attack value and the y-axis represents the item price. Each point on the plot will correspond to an individual item. \n",
    "  This visualization is designed to reveal whether items with higher attack values tend to be more expensive, allowing us to examine pricing trends and identify any correlations or outliers. This analysis directly supports our research question about item value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4  (50 points)\n",
    "\n",
    "Now, go ahead and try to scrape the features you need. \n",
    "\n",
    "Please show all the codes you have for web scrapping. Your current output data frame should include at least 4 features. (You do not need to scrape all features at this moment, although it is recommend to start earlier. Also, you can choose to not to use the ones you have scraped in the later analysis. No need to worry if you need to change anything later). **Please design your code in pipeline and clearly document each function.** See the Python Style Guide in Week 1 for proper documentation. It is also recommended to save the data you have scrapped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.4 Solution - Functions to extract Pal dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "import pylab as py\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Retrieve HTML content from a given URL\n",
    "\n",
    "    Args:\n",
    "        url (str): The webpage URL to fetch HTML from\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pal_links(html):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of links to pals from the HTML soup object.\n",
    "    \n",
    "    Args:\n",
    "        html (string): html text of a webpage\n",
    "        \n",
    "    Returns:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "    \"\"\"\n",
    "    # parse HTML content to create soup\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    pal_links = []  # Create an empty list to store full URLs\n",
    "\n",
    "    # Find all <div> elements with class pal\n",
    "    for pal_div in soup.find_all('div', class_='pal'):\n",
    "        \n",
    "        # Inside each <div class='pal'>, find the first <a> tag\n",
    "        a_tag = pal_div.find('a')\n",
    "        \n",
    "        # Check if the <a> tag exists and has an 'href' attribute\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            # Build the full link by appending the relative path to the base URL\n",
    "            full_link = 'https://palworld.gg' + a_tag['href']\n",
    "            pal_links.append(full_link)\n",
    "\n",
    "    return pal_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pal_data(pal_links):\n",
    "    \"\"\"\n",
    "    Given a list of Palworld page links, fetch each page and extract the pal's name and stats.\n",
    "\n",
    "    Args:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "\n",
    "    Returns:\n",
    "        pal_data (dictionary): dictionary containing a pal's name, link, and stats.\n",
    "    \"\"\"    \n",
    "    name, href, stats = [], [], []\n",
    "\n",
    "    for link in pal_links:\n",
    "        # Request the HTML content of each pal's page\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        pal_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Get the pal's name from the <h1> tag\n",
    "        name_tag = pal_soup.find('h1')\n",
    "        pal_name = name_tag.text.strip() if name_tag else 'Unknown'\n",
    "\n",
    "        # Dictionary to store stats like HP, Attack, etc.\n",
    "        stats_dict = {}\n",
    "\n",
    "        # Look for the section containing stats\n",
    "        stats_div = pal_soup.find('div', class_='stats')\n",
    "        if stats_div:\n",
    "            items_div = stats_div.find('div', class_='items')\n",
    "            if items_div:\n",
    "                # Loop through all stat items\n",
    "                for item in items_div.find_all('div', class_='item'):\n",
    "                    stat_name_tag = item.find('div', class_='name')\n",
    "                    stat_value_tag = item.find('div', class_='value')\n",
    "                    if stat_name_tag and stat_value_tag:\n",
    "                        stat_name = stat_name_tag.text.strip()\n",
    "                        stat_value = stat_value_tag.text.strip()\n",
    "                        stats_dict[stat_name] = stat_value\n",
    "\n",
    "        # Save the collected data into corresponding lists\n",
    "        name.append(pal_name)\n",
    "        href.append(link)\n",
    "        stats.append(stats_dict)\n",
    "        \n",
    "    pal_data = {'Name': name, 'Link': href, 'Stats': stats}\n",
    "\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rarity_info(pal_data):\n",
    "    \"\"\"\n",
    "    Adds rarity_level and rarity_name to each pal dictionary\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): contains pal info with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Rarity Level' and 'Rarity Name' fields added\n",
    "    \"\"\"\n",
    "    # empty lists to store rarity level and rarity name\n",
    "    rar_level, rar_name = [], []\n",
    "    \n",
    "    for link in pal_data['Link']:\n",
    "        html = get_html(link)  # FIXED: now using each Pal's page\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the rarity div\n",
    "        rarity_div = soup.find('div', class_='rarity')\n",
    "        rarity_level, rarity_name = None, None\n",
    "        if rarity_div:\n",
    "            lv_tag = rarity_div.find('div', class_='lv')\n",
    "            name_tag = rarity_div.find('div', class_='name')\n",
    "            if lv_tag and name_tag:\n",
    "                rarity_level = lv_tag.text.strip()\n",
    "                rarity_name = name_tag.text.strip()\n",
    "\n",
    "        rar_level.append(rarity_level)\n",
    "        rar_name.append(rarity_name)\n",
    "        \n",
    "    pal_data['Rarity Level'] = rar_level\n",
    "    pal_data['Rarity Name'] = rar_name\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_element_work(pal_data):\n",
    "    \"\"\"\n",
    "    Adds element and work suitability to pal_data\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): Dictionary, with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Element', 'Work Suitability' added\n",
    "    \n",
    "    \"\"\"\n",
    "    # empty lists to store element and work suitability\n",
    "    element, work = [], []\n",
    "    \n",
    "    # iterate through all links\n",
    "    for link in pal_data['Link']:\n",
    "        \n",
    "        # get HTML\n",
    "        html = get_html(link)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        # to store all elements of each pal\n",
    "        pal_element = []\n",
    "        \n",
    "        # get contents of first div with class elements\n",
    "        elements_div = soup.find('div', class_='elements')\n",
    "\n",
    "        # get text from each element and append to pal_element\n",
    "        for el in elements_div.find_all('div', class_='name'):\n",
    "                    element_text = el.text.strip()\n",
    "                    pal_element.append(element_text)\n",
    "        \n",
    "        # empty dict to store work suitability and level\n",
    "        work_level = {}\n",
    "\n",
    "        # get contents of first div with class works\n",
    "        works_div = soup.find('div', class_='works')\n",
    "\n",
    "        # iterates through contents of work_div with div and class item\n",
    "        for item in works_div.find_all('div', class_='item'):\n",
    "            \n",
    "            # only extracts displayed items\n",
    "            if 'display:none' not in item.get('style', ''):\n",
    "                if 'Lv' in item.text:\n",
    "                    work_suit, level = item.text.split('Lv')\n",
    "                    work_level[work_suit] = int(level)\n",
    "                else:\n",
    "                    work_suit = item.text.strip()\n",
    "                    work_level[work_suit] = ''\n",
    "        \n",
    "        # appends pal_element and work_level to the larger element and work lists\n",
    "        element.append(pal_element)\n",
    "        work.append(work_level)\n",
    "    \n",
    "    # adds to dictionary\n",
    "    pal_data['Element'] = element\n",
    "    pal_data['Work Suitability'] = work\n",
    "    \n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://palworld.gg/pals'\n",
    "html = get_html(url)\n",
    "pal_links = extract_pal_links(html)\n",
    "\n",
    "pal_data = fetch_pal_data(pal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_rarity_info(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_element_work(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pal_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xaviers Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# pal_data assumed available\n",
    "df = pd.DataFrame(pal_data)\n",
    "\n",
    "# Expand the dict in \"Stats\" to columns\n",
    "stats_df = df[\"Stats\"].apply(pd.Series)\n",
    "\n",
    "# Merge back and coerce numbers\n",
    "df = pd.concat([df.drop(columns=[\"Stats\"]), stats_df], axis=1)\n",
    "\n",
    "# Convert stat columns and rarity to numeric\n",
    "stat_cols = stats_df.columns.tolist()\n",
    "for c in stat_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"Rarity Level\"] = pd.to_numeric(df[\"Rarity Level\"], errors=\"coerce\")\n",
    "\n",
    "# Optional combined metric\n",
    "df[\"Total Stats\"] = df[stat_cols].sum(axis=1)\n",
    "\n",
    "# Drop rows missing target\n",
    "df = df.dropna(subset=[\"Rarity Level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "Xcols = stat_cols  # or stat_cols + [\"Total Stats\"] if you want to include the sum too\n",
    "\n",
    "single_results = []\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for col in Xcols:\n",
    "    X = df[[col]].values\n",
    "    y = df[\"Rarity Level\"].values\n",
    "    # cross-validated R^2\n",
    "    r2_scores = cross_val_score(LinearRegression(), X, y, scoring=\"r2\", cv=cv)\n",
    "    single_results.append({\n",
    "        \"Stat\": col,\n",
    "        \"CV_R2_mean\": np.mean(r2_scores),\n",
    "        \"CV_R2_std\": np.std(r2_scores)\n",
    "    })\n",
    "\n",
    "single_results_df = pd.DataFrame(single_results).sort_values(\"CV_R2_mean\", ascending=False)\n",
    "print(single_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[stat_cols].copy()\n",
    "y = df[\"Rarity Level\"].values\n",
    "\n",
    "# Train/test for a clean holdout evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Numeric pipeline\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, stat_cols)\n",
    "])\n",
    "\n",
    "reg_pipe = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "reg_pipe.fit(X_train, y_train)\n",
    "y_pred = reg_pipe.predict(X_test)\n",
    "print(\"Holdout R^2:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Permutation importance on the linear model\n",
    "perm = permutation_importance(reg_pipe, X_test, y_test, n_repeats=30, random_state=42)\n",
    "perm_importance = pd.DataFrame({\n",
    "    \"feature\": stat_cols,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "print(perm_importance)\n",
    "\n",
    "# RandomForestRegressor feature importance\n",
    "rf = Pipeline(steps=[\n",
    "    (\"pre\", pre),\n",
    "    (\"rf\", RandomForestRegressor(n_estimators=800, random_state=42))\n",
    "])\n",
    "rf.fit(X_train, y_train)\n",
    "rf_r2 = r2_score(y_test, rf.predict(X_test))\n",
    "print(\"RF Holdout R^2:\", rf_r2)\n",
    "\n",
    "rf_feat_imp = pd.Series(rf.named_steps[\"rf\"].feature_importances_, index=stat_cols)\\\n",
    "                .sort_values(ascending=False)\n",
    "print(rf_feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(data=df, x=\"Total Stats\", y=\"Rarity Level\", hue=\"Rarity Name\")\n",
    "plt.title(\"Total Stats vs Rarity Level\")\n",
    "plt.xlabel(\"Total Stats\")\n",
    "plt.ylabel(\"Rarity Level\")\n",
    "plt.legend(title=\"Rarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justin's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape weapon items from palworld.gg/items listing pages\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def _num_from_text(s):\n",
    "  m = re.search(r\"[-+]?\\d+(?:\\.\\d+)?\", s.replace(\",\", \"\"))\n",
    "  return float(m.group()) if m else np.nan\n",
    "\n",
    "def _field(text, label_patterns):\n",
    "  for lab in label_patterns:\n",
    "    m = re.search(lab + r\".{0,40}?([-+]?\\d+(?:\\.\\d+)?)\", text, re.I)\n",
    "    if m: \n",
    "      return _num_from_text(m.group(1))\n",
    "  return np.nan\n",
    "\n",
    "rows = []\n",
    "seen = set()\n",
    "for p in range(1, 20):  # pages 1..19\n",
    "  html = get_html(f\"https://palworld.gg/items?page={p}\")\n",
    "  soup = BeautifulSoup(html, \"html.parser\")\n",
    "  for a in soup.select('a[href*=\"/item/\"]'):\n",
    "    card = a\n",
    "    # walk up a few levels to get the full card text\n",
    "    for _ in range(4):\n",
    "      if card and card.parent: card = card.parent\n",
    "    text = card.get_text(\" \", strip=True) if card else a.get_text(\" \", strip=True)\n",
    "    if \"Weapon\" not in text:\n",
    "      continue\n",
    "    link = a.get(\"href\", \"\")\n",
    "    if link in seen:\n",
    "      continue\n",
    "    seen.add(link)\n",
    "    name = a.get_text(strip=True) or link.split(\"/\")[-1].replace(\"-\", \" \").title()\n",
    "    price = _field(text, [r\"\\bPrice\\b\", r\"Sell\\s*Price\", r\"\\bCost\\b\"])\n",
    "    durability = _field(text, [r\"\\bDurability\\b\"])\n",
    "    patk = _field(text, [r\"Physical\\s*Attack\", r\"Attack\\s*Power\", r\"\\bAtk\\b\"])\n",
    "    rows.append({\"name\": name, \"link\": f\"https://palworld.gg{link}\" if link.startswith(\"/\") else link,\n",
    "                 \"price\": price, \"durability\": durability, \"physical_attack\": patk})\n",
    "\n",
    "items_df = pd.DataFrame(rows)\n",
    "for c in [\"price\", \"durability\", \"physical_attack\"]:\n",
    "  items_df[c] = pd.to_numeric(items_df[c], errors=\"coerce\")\n",
    "items_df = items_df.dropna(subset=[\"price\", \"durability\", \"physical_attack\"])\n",
    "items_df = items_df[(items_df[\"price\"] > 0) & (items_df[\"durability\"] > 0) & (items_df[\"physical_attack\"] > 0)].reset_index(drop=True)\n",
    "items_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two scatterplots: price vs physical attack, and price vs durability\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "\n",
    "sns.regplot(data=items_df, x=\"physical_attack\", y=\"price\",\n",
    "            scatter_kws=dict(alpha=0.6, s=35), line_kws=dict(color=\"crimson\", alpha=0.9),\n",
    "            ax=axes[0])\n",
    "axes[0].set_title(\"Price vs Physical Attack\")\n",
    "axes[0].set_xlabel(\"Physical Attack\")\n",
    "axes[0].set_ylabel(\"Price\")\n",
    "\n",
    "sns.regplot(data=items_df, x=\"durability\", y=\"price\",\n",
    "            scatter_kws=dict(alpha=0.6, s=35), line_kws=dict(color=\"crimson\", alpha=0.9),\n",
    "            ax=axes[1])\n",
    "axes[1].set_title(\"Price vs Durability\")\n",
    "axes[1].set_xlabel(\"Durability\")\n",
    "axes[1].set_ylabel(\"Price\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
