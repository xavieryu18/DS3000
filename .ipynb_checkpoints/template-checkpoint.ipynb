{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the template for DS3000 Final data analysis project. Once you finish, please remove all my instructions. You do not need to exactly follow the structure in the template but please make sure you have all the components. Write your report in paragraphs. Only use bullet points when list something (eg: functions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS3000 Final Project \n",
    "#### Team number: 9\n",
    "- Xavier Yu, Vilasini Nathan, Justin Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pylab as py\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re, time, json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdunction\n",
    "\n",
    "- One or two paragraphs about the background of the project. eg: the backgound of PalWorld and why your analysis can be interesting\n",
    "- State your research questions. Limit the number of research questions to be one or two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "### Data Source\n",
    "\n",
    "- List the website you have scraped the data from.\n",
    "- List which information you have scraped\n",
    "- Describe what kind of cleaning you have done to the data\n",
    "\n",
    "### Webscraping and cleaning functions overview\n",
    "\n",
    "List all the functions you have written for webscraping and data cleaning. For each one, write one sentence to describe it. \n",
    "- `extract_soup()`\n",
    "    - build url and return soup object\n",
    "\n",
    "### Data overview\n",
    "\n",
    "- Show a couple of rows of the cleaned data you are going to use for the analysis\n",
    "- Which is your target value (if exists)\n",
    "- Give a general summary about the other features\n",
    "- Discuss if there is any potential problems about the data (eg: missing values, any features that you did not collect but may be important, any other concerns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for webscraping and cleaning. Make sure write full \n",
    "# docstrings for each function\n",
    "def extract_soup():\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Retrieve HTML content from a given URL\n",
    "\n",
    "    Args:\n",
    "        url (str): The webpage URL to fetch HTML from\n",
    "\n",
    "    Returns:\n",
    "        str: HTML content as a string.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pal_links(html):\n",
    "    \"\"\"\n",
    "    Extracts and returns a list of links to pals from the HTML soup object.\n",
    "    \n",
    "    Args:\n",
    "        html (string): html text of a webpage\n",
    "        \n",
    "    Returns:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "    \"\"\"\n",
    "    # parse HTML content to create soup\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    pal_links = []  # Create an empty list to store full URLs\n",
    "\n",
    "    # Find all <div> elements with class pal\n",
    "    for pal_div in soup.find_all('div', class_='pal'):\n",
    "        \n",
    "        # Inside each <div class='pal'>, find the first <a> tag\n",
    "        a_tag = pal_div.find('a')\n",
    "        \n",
    "        # Check if the <a> tag exists and has an 'href' attribute\n",
    "        if a_tag and a_tag.has_attr('href'):\n",
    "            # Build the full link by appending the relative path to the base URL\n",
    "            full_link = 'https://palworld.gg' + a_tag['href']\n",
    "            pal_links.append(full_link)\n",
    "\n",
    "    return pal_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pal_data(pal_links):\n",
    "    \"\"\"\n",
    "    Given a list of Palworld page links, fetch each page and extract the pal's name and stats.\n",
    "\n",
    "    Args:\n",
    "        pal_links (list): A list of full URLs to individual pal pages.\n",
    "\n",
    "    Returns:\n",
    "        pal_data (dictionary): dictionary containing a pal's name, link, and stats.\n",
    "    \"\"\"    \n",
    "    name, href, stats = [], [], []\n",
    "\n",
    "    for link in pal_links:\n",
    "        # Request the HTML content of each pal's page\n",
    "        response = requests.get(link)\n",
    "        if response.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        pal_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Get the pal's name from the <h1> tag\n",
    "        name_tag = pal_soup.find('h1')\n",
    "        pal_name = name_tag.text.strip() if name_tag else 'Unknown'\n",
    "\n",
    "        # Dictionary to store stats like HP, Attack, etc.\n",
    "        stats_dict = {}\n",
    "\n",
    "        # Look for the section containing stats\n",
    "        stats_div = pal_soup.find('div', class_='stats')\n",
    "        if stats_div:\n",
    "            items_div = stats_div.find('div', class_='items')\n",
    "            if items_div:\n",
    "                # Loop through all stat items\n",
    "                for item in items_div.find_all('div', class_='item'):\n",
    "                    stat_name_tag = item.find('div', class_='name')\n",
    "                    stat_value_tag = item.find('div', class_='value')\n",
    "                    if stat_name_tag and stat_value_tag:\n",
    "                        stat_name = stat_name_tag.text.strip()\n",
    "                        stat_value = stat_value_tag.text.strip()\n",
    "                        stats_dict[stat_name] = stat_value\n",
    "\n",
    "        # Save the collected data into corresponding lists\n",
    "        name.append(pal_name)\n",
    "        href.append(link)\n",
    "        stats.append(stats_dict)\n",
    "        \n",
    "    pal_data = {'Name': name, 'Link': href, 'Stats': stats}\n",
    "\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rarity_info(pal_data):\n",
    "    \"\"\"\n",
    "    Adds rarity_level and rarity_name to each pal dictionary\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): contains pal info with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Rarity Level' and 'Rarity Name' fields added\n",
    "    \"\"\"\n",
    "    # empty lists to store rarity level and rarity name\n",
    "    rar_level, rar_name = [], []\n",
    "    \n",
    "    for link in pal_data['Link']:\n",
    "        html = get_html(link)  # FIXED: now using each Pal's page\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the rarity div\n",
    "        rarity_div = soup.find('div', class_='rarity')\n",
    "        rarity_level, rarity_name = None, None\n",
    "        if rarity_div:\n",
    "            lv_tag = rarity_div.find('div', class_='lv')\n",
    "            name_tag = rarity_div.find('div', class_='name')\n",
    "            if lv_tag and name_tag:\n",
    "                rarity_level = lv_tag.text.strip()\n",
    "                rarity_name = name_tag.text.strip()\n",
    "\n",
    "        rar_level.append(rarity_level)\n",
    "        rar_name.append(rarity_name)\n",
    "        \n",
    "    pal_data['Rarity Level'] = rar_level\n",
    "    pal_data['Rarity Name'] = rar_name\n",
    "\n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_element_work(pal_data):\n",
    "    \"\"\"\n",
    "    Adds element and work suitability to pal_data\n",
    "    \n",
    "    Args:\n",
    "        pal_data (dictionary): Dictionary, with at least a 'Link' key.\n",
    "        \n",
    "    Returns:\n",
    "        pal_data (dictionary): 'Element', 'Work Suitability' added\n",
    "    \n",
    "    \"\"\"\n",
    "    # empty lists to store element and work suitability\n",
    "    element, work = [], []\n",
    "    \n",
    "    # iterate through all links\n",
    "    for link in pal_data['Link']:\n",
    "        \n",
    "        # get HTML\n",
    "        html = get_html(link)\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        # to store all elements of each pal\n",
    "        pal_element = []\n",
    "        \n",
    "        # get contents of first div with class elements\n",
    "        elements_div = soup.find('div', class_='elements')\n",
    "\n",
    "        # get text from each element and append to pal_element\n",
    "        for el in elements_div.find_all('div', class_='name'):\n",
    "                    element_text = el.text.strip()\n",
    "                    pal_element.append(element_text)\n",
    "        \n",
    "        # empty dict to store work suitability and level\n",
    "        work_level = {}\n",
    "\n",
    "        # get contents of first div with class works\n",
    "        works_div = soup.find('div', class_='works')\n",
    "\n",
    "        # iterates through contents of work_div with div and class item\n",
    "        for item in works_div.find_all('div', class_='item'):\n",
    "            \n",
    "            # only extracts displayed items\n",
    "            if 'display:none' not in item.get('style', ''):\n",
    "                if 'Lv' in item.text:\n",
    "                    work_suit, level = item.text.split('Lv')\n",
    "                    work_level[work_suit] = int(level)\n",
    "                else:\n",
    "                    work_suit = item.text.strip()\n",
    "                    work_level[work_suit] = ''\n",
    "        \n",
    "        # appends pal_element and work_level to the larger element and work lists\n",
    "        element.append(pal_element)\n",
    "        work.append(work_level)\n",
    "    \n",
    "    # adds to dictionary\n",
    "    pal_data['Element'] = element\n",
    "    pal_data['Work Suitability'] = work\n",
    "    \n",
    "    return pal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://palworld.gg/pals'\n",
    "html = get_html(url)\n",
    "pal_links = extract_pal_links(html)\n",
    "\n",
    "pal_data = fetch_pal_data(pal_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_rarity_info(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pal_data = add_element_work(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Link</th>\n",
       "      <th>Stats</th>\n",
       "      <th>Rarity Level</th>\n",
       "      <th>Rarity Name</th>\n",
       "      <th>Element</th>\n",
       "      <th>Work Suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anubis</td>\n",
       "      <td>https://palworld.gg/pal/anubis</td>\n",
       "      <td>{'HP': '120', 'Defense': '100', 'Crafting Spee...</td>\n",
       "      <td>10</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Earth]</td>\n",
       "      <td>{'Handiwork': 4, 'Mining': 3, 'Transporting': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arsox</td>\n",
       "      <td>https://palworld.gg/pal/arsox</td>\n",
       "      <td>{'HP': '85', 'Defense': '95', 'Crafting Speed'...</td>\n",
       "      <td>4</td>\n",
       "      <td>Common</td>\n",
       "      <td>[Fire]</td>\n",
       "      <td>{'Deforesting': 1, 'Kindling': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Astegon</td>\n",
       "      <td>https://palworld.gg/pal/astegon</td>\n",
       "      <td>{'HP': '100', 'Defense': '125', 'Crafting Spee...</td>\n",
       "      <td>9</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dragon, Dark]</td>\n",
       "      <td>{'Handiwork': 1, 'Mining': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Azurmane</td>\n",
       "      <td>https://palworld.gg/pal/azurmane</td>\n",
       "      <td>{'HP': '130', 'Defense': '110', 'Crafting Spee...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Electricity]</td>\n",
       "      <td>{'Gathering': 1, 'Generating Electricity': 4}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Azurobe</td>\n",
       "      <td>https://palworld.gg/pal/azurobe</td>\n",
       "      <td>{'HP': '110', 'Defense': '100', 'Crafting Spee...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Water, Dragon]</td>\n",
       "      <td>{'Watering': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Wumpo Botan</td>\n",
       "      <td>https://palworld.gg/pal/wumpo-botan</td>\n",
       "      <td>{'HP': '140', 'Defense': '110', 'Crafting Spee...</td>\n",
       "      <td>8</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Leaf]</td>\n",
       "      <td>{'Deforesting': 3, 'Handiwork': 2, 'Planting':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Xenogard</td>\n",
       "      <td>https://palworld.gg/pal/xenogard</td>\n",
       "      <td>{'HP': '110', 'Defense': '130', 'Crafting Spee...</td>\n",
       "      <td>9</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dragon]</td>\n",
       "      <td>{'Mining': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Xenolord</td>\n",
       "      <td>https://palworld.gg/pal/xenolord</td>\n",
       "      <td>{'HP': '130', 'Defense': '120', 'Crafting Spee...</td>\n",
       "      <td>8</td>\n",
       "      <td>Epic</td>\n",
       "      <td>[Dark, Dragon]</td>\n",
       "      <td>{'Gathering': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Xenovader</td>\n",
       "      <td>https://palworld.gg/pal/xenovader</td>\n",
       "      <td>{'HP': '90', 'Defense': '85', 'Crafting Speed'...</td>\n",
       "      <td>7</td>\n",
       "      <td>Rare</td>\n",
       "      <td>[Dark]</td>\n",
       "      <td>{'Deforesting': 2, 'Transporting': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Yakumo</td>\n",
       "      <td>https://palworld.gg/pal/yakumo</td>\n",
       "      <td>{'HP': '85', 'Defense': '85', 'Crafting Speed'...</td>\n",
       "      <td>4</td>\n",
       "      <td>Common</td>\n",
       "      <td>[Normal]</td>\n",
       "      <td>{'Gathering': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name                                 Link  \\\n",
       "0         Anubis       https://palworld.gg/pal/anubis   \n",
       "1          Arsox        https://palworld.gg/pal/arsox   \n",
       "2        Astegon      https://palworld.gg/pal/astegon   \n",
       "3       Azurmane     https://palworld.gg/pal/azurmane   \n",
       "4        Azurobe      https://palworld.gg/pal/azurobe   \n",
       "..           ...                                  ...   \n",
       "220  Wumpo Botan  https://palworld.gg/pal/wumpo-botan   \n",
       "221     Xenogard     https://palworld.gg/pal/xenogard   \n",
       "222     Xenolord     https://palworld.gg/pal/xenolord   \n",
       "223    Xenovader    https://palworld.gg/pal/xenovader   \n",
       "224       Yakumo       https://palworld.gg/pal/yakumo   \n",
       "\n",
       "                                                 Stats Rarity Level  \\\n",
       "0    {'HP': '120', 'Defense': '100', 'Crafting Spee...           10   \n",
       "1    {'HP': '85', 'Defense': '95', 'Crafting Speed'...            4   \n",
       "2    {'HP': '100', 'Defense': '125', 'Crafting Spee...            9   \n",
       "3    {'HP': '130', 'Defense': '110', 'Crafting Spee...            7   \n",
       "4    {'HP': '110', 'Defense': '100', 'Crafting Spee...            7   \n",
       "..                                                 ...          ...   \n",
       "220  {'HP': '140', 'Defense': '110', 'Crafting Spee...            8   \n",
       "221  {'HP': '110', 'Defense': '130', 'Crafting Spee...            9   \n",
       "222  {'HP': '130', 'Defense': '120', 'Crafting Spee...            8   \n",
       "223  {'HP': '90', 'Defense': '85', 'Crafting Speed'...            7   \n",
       "224  {'HP': '85', 'Defense': '85', 'Crafting Speed'...            4   \n",
       "\n",
       "    Rarity Name          Element  \\\n",
       "0          Epic          [Earth]   \n",
       "1        Common           [Fire]   \n",
       "2          Epic   [Dragon, Dark]   \n",
       "3          Rare    [Electricity]   \n",
       "4          Rare  [Water, Dragon]   \n",
       "..          ...              ...   \n",
       "220        Epic           [Leaf]   \n",
       "221        Epic         [Dragon]   \n",
       "222        Epic   [Dark, Dragon]   \n",
       "223        Rare           [Dark]   \n",
       "224      Common         [Normal]   \n",
       "\n",
       "                                      Work Suitability  \n",
       "0     {'Handiwork': 4, 'Mining': 3, 'Transporting': 2}  \n",
       "1                    {'Deforesting': 1, 'Kindling': 2}  \n",
       "2                        {'Handiwork': 1, 'Mining': 4}  \n",
       "3        {'Gathering': 1, 'Generating Electricity': 4}  \n",
       "4                                      {'Watering': 3}  \n",
       "..                                                 ...  \n",
       "220  {'Deforesting': 3, 'Handiwork': 2, 'Planting':...  \n",
       "221                                      {'Mining': 3}  \n",
       "222                                   {'Gathering': 1}  \n",
       "223              {'Deforesting': 2, 'Transporting': 2}  \n",
       "224                                   {'Gathering': 2}  \n",
       "\n",
       "[225 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Items/Weapons Scraper — Setup\n",
    "\n",
    "BASE_ITEMS_URL = \"https://palworld.gg/items\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use your existing get_html(url) if it exists; otherwise define a light fallback with retries.\n",
    "if \"get_html\" not in globals():\n",
    "    def get_html(url, retries=3, backoff=0.6):\n",
    "        last_err = None\n",
    "        for attempt in range(1, retries+1):\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=15)\n",
    "                resp.raise_for_status()\n",
    "                return resp.text\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                time.sleep(backoff * attempt)\n",
    "        raise last_err\n",
    "\n",
    "def _to_num(x):\n",
    "    \"\"\"Best-effort numeric conversion for strings like '1,200', '120 ATK', '-'.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"[^\\d\\.\\-]\", \"\", s)  # keep digits/dot/minus\n",
    "    if s in {\"\", \".\", \"-\"}:\n",
    "        return None\n",
    "    try:\n",
    "        return int(s) if re.fullmatch(r\"-?\\d+\", s) else float(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _field(text, patterns):\n",
    "    \"\"\"\n",
    "    Find a numeric field following a label in flattened text.\n",
    "    Returns first numeric-like value found after the label.\n",
    "    \"\"\"\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat + r\"\\s*[:\\-]?\\s*([^\\n\\r]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            val = m.group(1).strip()\n",
    "            nm = re.search(r\"-?\\d[\\d,\\.]*\", val)\n",
    "            return _to_num(nm.group(0)) if nm else _to_num(val)\n",
    "    return None\n",
    "\n",
    "def _field_text(text, patterns):\n",
    "    \"\"\"Like _field, but returns raw text after the label (for categorical fields).\"\"\"\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat + r\"\\s*[:\\-]?\\s*([^\\n\\r]+)\", text, flags=re.I)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def _flatten_text(el):\n",
    "    return re.sub(r\"\\s+\", \" \", el.get_text(\" \", strip=True)).strip()\n",
    "\n",
    "KNOWN_TYPES = {\"Weapon\",\"Material\",\"Consumable\",\"Ammo\",\"Essential\",\n",
    "               \"Food\",\"Blueprint\",\"Armor\",\"Glider\",\"Spheres\",\"Accessory\"}\n",
    "\n",
    "def _norm_name(s: str) -> str:\n",
    "    \"\"\"Normalize a name for matching: lowercase, strip accents, alnum only.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower()\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
    "\n",
    "def _slugify_name(s: str) -> str:\n",
    "    \"\"\"Slug suitable for palworld.gg/items/<slug> from a display name.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # strip accents\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = s.lower().strip()\n",
    "    # replace non-alnum with dashes, collapse dashes\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"-\", s)\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s\n",
    "\n",
    "def _probe_url_exists(url: str, timeout: float = 10.0) -> bool:\n",
    "    \"\"\"HEAD/GET to check if a constructed detail URL exists.\"\"\"\n",
    "    try:\n",
    "        r = requests.head(url, allow_redirects=True, timeout=timeout)\n",
    "        if r.status_code == 405:  # some sites block HEAD\n",
    "            r = requests.get(url, allow_redirects=True, timeout=timeout)\n",
    "        return 200 <= r.status_code < 400\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_items_listing_html(html):\n",
    "    \"\"\"\n",
    "    Parse a single /items page into item dicts, with reliable link inference.\n",
    "    - Anchor indices:\n",
    "        1) normalized label -> href\n",
    "        2) href slug (last path segment) -> href\n",
    "    - Extract fields from listing text\n",
    "    - Attach link by (label match) -> (slug match) -> (partial) -> (constructed/probed)\n",
    "    - De-duplicate within the page on (name,item_type)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Build anchor indices\n",
    "    anchor_by_label = {}  # norm(label) -> href\n",
    "    anchor_by_slug  = {}  # slug(from href) -> href\n",
    "\n",
    "    for a in soup.select('a[href^=\"/items/\"], a[href*=\"/items/\"]'):\n",
    "        href = a.get(\"href\")\n",
    "        if not href:\n",
    "            continue\n",
    "        label = a.get_text(\" \", strip=True) or \"\"\n",
    "        # absolute href\n",
    "        abs_href = (\"https://palworld.gg\" + href) if href.startswith(\"/\") else href\n",
    "\n",
    "        # label index\n",
    "        nn = _norm_name(label)\n",
    "        if nn and nn not in anchor_by_label:\n",
    "            anchor_by_label[nn] = abs_href\n",
    "\n",
    "        # slug index (last segment of path)\n",
    "        try:\n",
    "            path_seg = href.split(\"?\")[0].rstrip(\"/\").split(\"/\")[-1]\n",
    "            slug = _slugify_name(path_seg)\n",
    "            if slug and slug not in anchor_by_slug:\n",
    "                anchor_by_slug[slug] = abs_href\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Flatten page text\n",
    "    texts = [_flatten_text(x) for x in soup.find_all([\"p\",\"div\",\"span\",\"li\",\"h2\",\"h3\"])]\n",
    "    texts = [t for t in texts if t]\n",
    "\n",
    "    records = []\n",
    "    i = 0\n",
    "    while i < len(texts):\n",
    "        name = texts[i]\n",
    "        if i + 1 >= len(texts):\n",
    "            i += 1\n",
    "            continue\n",
    "        item_type = texts[i+1]\n",
    "        if item_type not in KNOWN_TYPES:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # find the end of this block (next name+type pair)\n",
    "        j = i + 2\n",
    "        while j < len(texts):\n",
    "            if j + 1 < len(texts) and texts[j+1] in KNOWN_TYPES:\n",
    "                break\n",
    "            j += 1\n",
    "\n",
    "        block = \" \".join(texts[i:j])\n",
    "        rec = {\n",
    "            \"name\": name, \"item_type\": item_type,\n",
    "            \"rarity\": None, \"rank\": None, \"price\": None, \"weight\": None,\n",
    "            \"durability\": None, \"magazine\": None, \"physical_attack\": None,\n",
    "            \"link\": None\n",
    "        }\n",
    "\n",
    "        rec[\"price\"]           = _field(block, [r\"\\bPrice\\b\", r\"Sell\\s*Price\", r\"\\bCost\\b\"])\n",
    "        rec[\"durability\"]      = _field(block, [r\"\\bDurability\\b\"])\n",
    "        rec[\"physical_attack\"] = _field(block, [r\"Physical\\s*Attack\", r\"Attack\\s*Power\", r\"\\bATK\\b\"])\n",
    "        rec[\"weight\"]          = _field(block, [r\"\\bWeight\\b\"])\n",
    "        rec[\"magazine\"]        = _field(block, [r\"\\bMagazine\\b\"])\n",
    "        rec[\"rank\"]            = _field(block, [r\"\\bRank\\b\"])\n",
    "        rar_num                = _field(block, [r\"\\bRarity\\b\"])\n",
    "        rar_txt                = _field_text(block, [r\"\\bRarity\\b\"])\n",
    "        rec[\"rarity\"]          = rar_num if rar_num is not None else rar_txt\n",
    "\n",
    "        # Link attachment strategies\n",
    "        nn = _norm_name(name)\n",
    "        slug_from_name = _slugify_name(name)\n",
    "\n",
    "        # 1) exact label match\n",
    "        link = anchor_by_label.get(nn)\n",
    "\n",
    "        # 2) slug match (href last segment)\n",
    "        if not link and slug_from_name:\n",
    "            link = anchor_by_slug.get(slug_from_name)\n",
    "\n",
    "        # 3) partial label match\n",
    "        if not link and nn:\n",
    "            for lab, href in anchor_by_label.items():\n",
    "                if nn in lab:\n",
    "                    link = href\n",
    "                    break\n",
    "\n",
    "        # 4) constructed/probed fallback: https://palworld.gg/items/<slug>\n",
    "        if not link and slug_from_name:\n",
    "            cand = f\"https://palworld.gg/items/{slug_from_name}\"\n",
    "            if _probe_url_exists(cand):\n",
    "                link = cand\n",
    "\n",
    "        rec[\"link\"] = link\n",
    "        records.append(rec)\n",
    "        i = j\n",
    "\n",
    "    # Per-page de-dup on (name,item_type); preserve any non-null link\n",
    "    df = pd.DataFrame(records)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"name\",\"item_type\"], kind=\"stable\")\n",
    "        if \"link\" in df.columns:\n",
    "            df[\"link\"] = df.groupby([\"name\",\"item_type\"])[\"link\"].transform(\n",
    "                lambda s: next((x for x in s if pd.notna(x) and x), pd.NA)\n",
    "            )\n",
    "        df = df.drop_duplicates(subset=[\"name\",\"item_type\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df.to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse item detail page (enrichment)\n",
    "def parse_item_detail_html(html):\n",
    "    \"\"\"\n",
    "    Returns dict with: rarity, rank, price, weight, durability, magazine,\n",
    "    physical_attack, recipe (list of {ingredient, qty})\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = _flatten_text(soup)\n",
    "    out = {\n",
    "        \"rarity\": _field(text, [r\"\\bRarity\\b\"]) or _field_text(text, [r\"\\bRarity\\b\"]),\n",
    "        \"rank\": _field(text, [r\"\\bRank\\b\"]),\n",
    "        \"price\": _field(text, [r\"\\bPrice\\b\", r\"Sell\\s*Price\", r\"\\bCost\\b\"]),\n",
    "        \"weight\": _field(text, [r\"\\bWeight\\b\"]),\n",
    "        \"durability\": _field(text, [r\"\\bDurability\\b\"]),\n",
    "        \"magazine\": _field(text, [r\"\\bMagazine\\b\"]),\n",
    "        \"physical_attack\": _field(text, [r\"Physical\\s*Attack\", r\"Attack\\s*Power\", r\"\\bATK\\b\"]),\n",
    "        \"recipe\": []\n",
    "    }\n",
    "\n",
    "    # Best-effort recipe section capture\n",
    "    m = re.search(r\"\\bRecipe\\b(.*?)(Rarity|Rank|Price|Weight|Durability|Magazine|Physical\\s*Attack|$)\", text, flags=re.I)\n",
    "    if m:\n",
    "        seg = m.group(1)\n",
    "        tokens = re.split(r\"[,/]\", seg)\n",
    "        for tk in tokens:\n",
    "            tk = tk.strip()\n",
    "            if not tk:\n",
    "                continue\n",
    "            nm = re.search(r\"(.*?)(-?\\d[\\d\\.]*)\\s*$\", tk)\n",
    "            if nm:\n",
    "                ingr = nm.group(1).strip()\n",
    "                qty = _to_num(nm.group(2))\n",
    "                if ingr:\n",
    "                    out[\"recipe\"].append({\"ingredient\": ingr, \"qty\": qty})\n",
    "            else:\n",
    "                out[\"recipe\"].append({\"ingredient\": tk, \"qty\": None})\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level scrapers: paginate, filter to weapons, and enrich\n",
    "def scrape_items_page(page:int=1):\n",
    "    html = get_html(f\"{BASE_ITEMS_URL}?page={page}\")\n",
    "    items = parse_items_listing_html(html)\n",
    "    dfp = pd.DataFrame(items)\n",
    "    if not dfp.empty:\n",
    "        if \"link\" in dfp.columns:\n",
    "            dfp[\"link\"] = dfp.groupby([\"name\",\"item_type\"])[\"link\"].transform(\n",
    "                lambda s: next((x for x in s if pd.notna(x) and x), pd.NA)\n",
    "            )\n",
    "        dfp = dfp.drop_duplicates(subset=[\"name\",\"item_type\"], keep=\"first\").reset_index(drop=True)\n",
    "    return dfp\n",
    "\n",
    "def scrape_all_items(max_pages:int=50, sleep_sec:float=0.6):\n",
    "    frames = []\n",
    "    for p in range(1, max_pages+1):\n",
    "        dfp = scrape_items_page(p)\n",
    "        if dfp.empty:\n",
    "            break\n",
    "        frames.append(dfp)\n",
    "        time.sleep(sleep_sec)\n",
    "    if not frames:\n",
    "        cols = [\"name\",\"item_type\",\"rarity\",\"rank\",\"price\",\"weight\",\n",
    "                \"durability\",\"magazine\",\"physical_attack\",\"link\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    if \"link\" in df.columns:\n",
    "        df[\"link\"] = df.groupby([\"name\",\"item_type\"])[\"link\"].transform(\n",
    "            lambda s: next((x for x in s if pd.notna(x) and x), pd.NA)\n",
    "        )\n",
    "    df = df.drop_duplicates(subset=[\"name\",\"item_type\"], keep=\"first\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def enrich_items_with_details(df_items, sleep_sec:float=0.4):\n",
    "    df = df_items.copy()\n",
    "    if \"recipe_json\" not in df.columns:\n",
    "        df[\"recipe_json\"] = pd.NA\n",
    "\n",
    "    mask = (df[\"item_type\"].str.lower() == \"weapon\")\n",
    "    idxs = df[mask & df[\"link\"].notna()].index\n",
    "\n",
    "    for i in idxs:\n",
    "        url = df.at[i, \"link\"]\n",
    "        try:\n",
    "            html = get_html(url)\n",
    "            det = parse_item_detail_html(html)\n",
    "            for k in [\"rarity\",\"rank\",\"price\",\"weight\",\"durability\",\"magazine\",\"physical_attack\"]:\n",
    "                if pd.isna(df.at[i, k]) or df.at[i, k] in [\"\", None]:\n",
    "                    df.at[i, k] = det.get(k, df.at[i, k])\n",
    "            df.at[i, \"recipe_json\"] = json.dumps(det.get(\"recipe\", []), ensure_ascii=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    for c in [\"price\",\"weight\",\"durability\",\"magazine\",\"physical_attack\",\"rank\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def scrape_weapons_palworld_items(max_pages:int=40, enrich:bool=True, sleep_sec:float=0.6):\n",
    "    \"\"\"\n",
    "    End-to-end weapon scraper:\n",
    "      1) crawl /items pages\n",
    "      2) filter weapons\n",
    "      3) optional enrich with detail pages\n",
    "      4) return tidy DataFrame\n",
    "    \"\"\"\n",
    "    all_items = scrape_all_items(max_pages=max_pages, sleep_sec=sleep_sec)\n",
    "    if all_items.empty:\n",
    "        cols = [\"name\",\"item_type\",\"rarity\",\"rank\",\"price\",\"weight\",\"durability\",\n",
    "                \"magazine\",\"physical_attack\",\"link\",\"recipe_json\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    weapons = all_items[all_items[\"item_type\"].str.lower() == \"weapon\"].reset_index(drop=True)\n",
    "    if enrich:\n",
    "        weapons = enrich_items_with_details(weapons, sleep_sec=min(0.6, sleep_sec))\n",
    "\n",
    "    # Keep rows with vendor price and at least one capability metric\n",
    "    keep = (\n",
    "        weapons[\"price\"].notna()\n",
    "        & (\n",
    "            weapons[\"physical_attack\"].notna()\n",
    "            | weapons[\"durability\"].notna()\n",
    "            | weapons[\"weight\"].notna()\n",
    "        )\n",
    "    )\n",
    "    weapons = weapons[keep].reset_index(drop=True)\n",
    "\n",
    "    # Canonical column order\n",
    "    cols = [\"name\",\"item_type\",\"rarity\",\"rank\",\"price\",\"weight\",\n",
    "            \"durability\",\"magazine\",\"physical_attack\",\"link\",\"recipe_json\"]\n",
    "    for c in cols:\n",
    "        if c not in weapons.columns:\n",
    "            weapons[c] = pd.NA\n",
    "    return weapons[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_type</th>\n",
       "      <th>rarity</th>\n",
       "      <th>rank</th>\n",
       "      <th>price</th>\n",
       "      <th>weight</th>\n",
       "      <th>durability</th>\n",
       "      <th>magazine</th>\n",
       "      <th>physical_attack</th>\n",
       "      <th>link</th>\n",
       "      <th>recipe_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assault Rifle</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>82400</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Axe4</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>33000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bat</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crossbow</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25200</td>\n",
       "      <td>13.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decal Gun 1</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decal Gun 2</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decal Gun 3</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decal Gun 4</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decal Gun 5</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>33000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Double-Barreled Shotgun</td>\n",
       "      <td>Weapon</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>55200</td>\n",
       "      <td>24.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name item_type  rarity  rank  price  weight  durability  \\\n",
       "0            Assault Rifle    Weapon       0     3  82400    15.0      3000.0   \n",
       "1                     Axe4    Weapon       0     3  33000    25.0       400.0   \n",
       "2                      Bat    Weapon       0     1     80     3.0       150.0   \n",
       "3                 Crossbow    Weapon       0     2  25200    13.0       300.0   \n",
       "4              Decal Gun 1    Weapon       0     2  33000     8.0         NaN   \n",
       "5              Decal Gun 2    Weapon       0     2  33000     8.0         NaN   \n",
       "6              Decal Gun 3    Weapon       0     2  33000     8.0         NaN   \n",
       "7              Decal Gun 4    Weapon       0     2  33000     8.0         NaN   \n",
       "8              Decal Gun 5    Weapon       0     2  33000     8.0         NaN   \n",
       "9  Double-Barreled Shotgun    Weapon       0     3  55200    24.0       200.0   \n",
       "\n",
       "   magazine  physical_attack  link recipe_json  \n",
       "0      20.0            320.0  <NA>        <NA>  \n",
       "1       NaN             75.0  <NA>        <NA>  \n",
       "2       NaN             50.0  <NA>        <NA>  \n",
       "3       1.0            280.0  <NA>        <NA>  \n",
       "4      99.0              NaN  <NA>        <NA>  \n",
       "5      99.0              NaN  <NA>        <NA>  \n",
       "6      99.0              NaN  <NA>        <NA>  \n",
       "7      99.0              NaN  <NA>        <NA>  \n",
       "8      99.0              NaN  <NA>        <NA>  \n",
       "9       2.0            190.0  <NA>        <NA>  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute scrape and save\n",
    "weapons_df = scrape_weapons_palworld_items(max_pages=40, enrich=True, sleep_sec=0.5)\n",
    "weapons_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "### Visualization functions overview\n",
    "List all the functions you have written for visualization. For each one, write one sentence to describe it. \n",
    "- `make_hist()`\n",
    "    - Generate a histogram with given data and feature\n",
    " \n",
    "### Visualization results\n",
    "- Present 3-4 data visualizations.\n",
    "- For each visualization, you need to include title, xlabel, ylabel, legend (if necessary)\n",
    "- For each visualization, explain why you make this data visualization (how it related to your research question) and explain what you have learned from this visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for visualization. Make sure write full \n",
    "# docstrings for each function\n",
    "def make_hist(df, y_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to get each data visualization in separate code chunks. \n",
    "# Interpret the figures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Modeling functions overview\n",
    "List all the functions you have written for modeling. For each one, write one sentence to describe it. \n",
    "- `fit_linear()`\n",
    "    - fit a linear model to the data and output the r2, slope and intercept\n",
    "\n",
    "### Model results\n",
    "\n",
    "- Present 2-3 models for the analysis.\n",
    "- Explain any pre-processing steps you have done (eg: scaling, polynomial, dummy features)\n",
    "- For each model, explain why you think this model is suitable and what metrics you want to use to evaluate the model\n",
    "    - If it is a classification model, you need to present the confusion matrix, calculate the accuracy, sensitivity and specificity with cross-validation\n",
    "    - If it is a regression model, you need to present the r2 and MSE with cross-validation\n",
    "    - If it is a linear regression model/multiple linear regression model, you need to interpret the meaning of the coefficient with the full data\n",
    "    - If it is a decision tree model, you need to plot the tree with the full data\n",
    "    - If it is a random forest model, you need to present the feature importance plot with the full data\n",
    "    - If it is a PCA, you need to explain how to select the number of components and interpret the key features in the first two components\n",
    "    - If it is a clustering, you need explain how to select the number of clustering and summarize the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the functions you have for modeling. Make sure write full \n",
    "# docstrings for each function\n",
    "def fit_linear(df, y_feat, x_feat):\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to run functions to fit each model in separate code chunks. \n",
    "# Interpret the model results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- One or two paragraphs to summarize your findings in the modeling sections and do the models answer your research question?\r",
    "- Any other potential thing you can do with the analysis (eg: include more features, get more data, try some other models etc.)\n",
    "- List the contribution for each group member."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
